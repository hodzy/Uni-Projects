{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first method tried\n",
    "# method based on Bag of visual words and SIFT\n",
    "# step 1: construct the dictionary of the visual words\n",
    "# step 2: classify the images using the clusters from the dictionary (training)\n",
    "# step 3: classify the images using the clusters from the dictionary (test)\n",
    "\n",
    "# images were not rotated, wanted to test the method on normal images, the results were not promising, discussed the method\n",
    "# with the supervisor, and he discouraged such complex ideas (SIFT) and wanted simpler ideas, as the presentation \n",
    "# is limited to 15 minutes and we had other problems that we had to deal with (translation, resizing)\n",
    "###\n",
    "# so the next one was using contouring to find the angle\n",
    "\n",
    "# with thresholding \n",
    "# sample length:  9708\n",
    "# accuracy:  53.56407086938607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import math\n",
    "import os, os.path\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of best features to retain to 15 for each image\n",
    "sift = cv2.xfeatures2d.SIFT_create(nfeatures = 15)\n",
    "# sift = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# here descriptors are found to be used to find clusters (bag of visual words)\n",
    "\n",
    "imgs = []\n",
    "path = \".\\MNIST_Dataset_JPG_format\\MNIST_JPG_training\"\n",
    "\n",
    "exclude_count = 0\n",
    "total_desc_samples = []\n",
    "\n",
    "valid_images = [\".jpg\"]\n",
    "for i in range(10):\n",
    "    total_descriptors = 0\n",
    "    chosen_per_folder = 0\n",
    "    imgs_subset=[]\n",
    "    path = path + \"\\\\\" + str(i)\n",
    "    print(i)\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        if ext.lower() not in valid_images:\n",
    "            continue\n",
    "        image = cv2.imread(os.path.join(path,f))\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "\n",
    "        if(len(keypoints) > 0):\n",
    "            total_desc_samples.extend(descriptors.tolist())\n",
    "\n",
    "    path = path.rsplit('\\\\',1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634570367.1476102\n",
      "Training time = 54.478182315826416\n"
     ]
    }
   ],
   "source": [
    "# Bag of visual words\n",
    "# here we group to the descriptors to clusters, so that these clsuters will be used to classify the descriptors of the\n",
    "# images (training and test) we have later\n",
    "\n",
    "\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# kmeans = KMeans(n_clusters=40, random_state=0).fit(total_desc_samples)\n",
    "# ncentroids = 1024\n",
    "# niter = 20\n",
    "# verbose = True\n",
    "# d = x.shape[1]\n",
    "# kmeans = faiss.Kmeans(d, ncentroids, niter=niter, verbose=verbose)\n",
    "# kmeans.train(x)\n",
    "\n",
    "total_desc_samples = np.array(total_desc_samples)\n",
    "\n",
    "k = 200\n",
    "n_init = 10\n",
    "max_iter = 300\n",
    "kmeans_faiss = faiss.Kmeans(d=total_desc_samples.shape[1], k=k, niter=max_iter, nredo=n_init)\n",
    "s = time.time()\n",
    "print(s)\n",
    "kmeans_faiss.train(total_desc_samples.astype(np.float32))\n",
    "e = time.time()\n",
    "print(\"Training time = {}\".format(e - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENT ADDED LATER: time taken for the faiss algorithm to finish (signicantly less than the tradiditonal kmeans in our\n",
    "# usecase, i think traditional kmeans took around 10 times longer)\n",
    "\n",
    "# Training time = 5.611945152282715 (clusters = 40, max_iter = 300)\n",
    "# Training time = 8.646806001663208 (clusters = 40, max_iter = 500)\n",
    "# Training time = 34.214611291885376 (clusters = 100, max_iter = 500)\n",
    "# Training time = 89.70109343528748 (best time circa: 80) (clusters = 200, max_iter = 500) \n",
    "# Training time = 182.177996635437 (best time= 154.5881745815277) (clusters = 300, max_iter = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# now We check the feature descriptors (keypoints) generated from each image in the training set against\n",
    "# the vocabulary to determine how many of the feature descriptors, saved in the last step, are present in each image.\n",
    "# and these bin assignments are used later to classify the images\n",
    "\n",
    "\n",
    "training_data = []\n",
    "training_label = []\n",
    "path = \".\\MNIST_Dataset_JPG_format\\MNIST_JPG_training\"\n",
    "valid_images = [\".jpg\"]\n",
    "\n",
    "for i in range(10):\n",
    "    path = path + \"\\\\\" + str(i)\n",
    "#     print(path)\n",
    "    print(i)\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        if ext.lower() not in valid_images:\n",
    "            continue\n",
    "        \n",
    "        # read the image\n",
    "        image = cv2.imread(os.path.join(path,f))\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        \n",
    "        # for each descriptor, find the nearest cluster\n",
    "        if len(keypoints) != 0:\n",
    "            bin_assignments = kmeans_faiss.index.search(descriptors.astype(np.float32), 1)[1]\n",
    "            bin_assignments = bin_assignments.reshape(1, bin_assignments.shape[0])\n",
    "            \n",
    "            # sort, improves the accuarcy of the model\n",
    "            bin_assignments[0].sort()\n",
    "            \n",
    "            training_data.append(bin_assignments[0])\n",
    "            training_label.append(i)\n",
    "     \n",
    "    path = path.rsplit('\\\\',1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# we check if any image in our dataset have less or more than 15 descriptors\n",
    "# mainly we check if they have less than 15 (as the SIFT algorithm generate less than 15 for some images)\n",
    "\n",
    "false_samples =0\n",
    "for i in range(len(training_data)):\n",
    "    if (len(training_data[i]) < 15):\n",
    "        elements=[1000] * (15 - len(training_data[i]))\n",
    "        training_data[i] = np.append(training_data[i], elements, 0)\n",
    "    \n",
    "    elif (len(training_data[i]) > 15):\n",
    "        index=[range(15, len(training_data[i]),1)]\n",
    "        training_data[i] = np.delete(training_data[i], index)\n",
    "#     print(training_data[i])\n",
    "    \n",
    "    \n",
    "    if(len(training_data[i]) != 15):\n",
    "        false_samples += 1\n",
    "\n",
    "print (false_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=15, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=15, random_state=0)\n",
    "\n",
    "clf.fit(training_data, training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# here the steps of finding the bin assignments and limiting or adding psedo descriptors to the the asssignments to one step\n",
    "\n",
    "path = \".\\MNIST_Dataset_JPG_format\\MNIST_JPG_testing\"\n",
    "test_data = []\n",
    "test_labels = []\n",
    "\n",
    "for i in range(10):\n",
    "    path = path + \"\\\\\" + str(i)\n",
    "    print(i)\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1]\n",
    "        if ext.lower() not in valid_images:\n",
    "            continue\n",
    "        image = cv2.imread(os.path.join(path,f))\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        if len(keypoints) != 0:\n",
    "#             bin_assignments = kmeans.predict(descriptors)\n",
    "            bin_assignments = kmeans_faiss.index.search(descriptors.astype(np.float32), 1)[1]\n",
    "            bin_assignments = bin_assignments.reshape(1, bin_assignments.shape[0])[0]\n",
    "#             print(bin_assignments[0])\n",
    "            if (len(bin_assignments) < 15):\n",
    "                elements=[1000] * (15 - len(bin_assignments))\n",
    "                bin_assignments = np.append(bin_assignments, elements, 0)\n",
    "    \n",
    "            elif (len(bin_assignments) > 15):\n",
    "                index=[range(15, len(bin_assignments), 1)]\n",
    "                bin_assignments = np.delete(bin_assignments, index)\n",
    "            bin_assignments.sort()\n",
    "            test_data.append(bin_assignments)\n",
    "            test_labels.append(i)\n",
    "            \n",
    "    path = path.rsplit('\\\\',1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of test labels:  9708\n",
      "len of test data:  9708\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "wrong_len = 0\n",
    "print(\"len of test labels: \", len(test_labels))\n",
    "print(\"len of test data: \", len(test_data))\n",
    "for sample in test_data:\n",
    "    if len(sample) != 15:\n",
    "        wrong_len += 1\n",
    "        print(sample[0])\n",
    "print(wrong_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = clf.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 7 6 4]\n"
     ]
    }
   ],
   "source": [
    "print(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errors: 4508\n",
      "sample length:  9708\n",
      "accuracy:  53.56407086938607\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for i in range(len(test_results)):\n",
    "    if test_results[i] != test_labels[i]:\n",
    "        error += 1\n",
    "print(\"errors:\", error)\n",
    "print(\"sample length: \", len(test_labels))\n",
    "print(\"accuracy: \", (len(test_labels)-error)/len(test_labels)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
